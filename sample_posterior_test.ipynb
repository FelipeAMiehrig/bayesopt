{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", gpytorch.utils.warnings.NumericalWarning)\n",
    "\n",
    "#%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 50)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_KEOPS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "\n",
    "        if HAVE_KEOPS:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.keops.RBFKernel())\n",
    "        else:\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    train_x = train_x.cuda()\n",
    "    train_y = train_y.cuda()\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 0.990   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 0.960   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 0.929   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 0.896   lengthscale: 0.554   noise: 0.554\n",
      "Iter 5/50 - Loss: 0.859   lengthscale: 0.513   noise: 0.513\n",
      "Iter 6/50 - Loss: 0.816   lengthscale: 0.474   noise: 0.474\n",
      "Iter 7/50 - Loss: 0.769   lengthscale: 0.438   noise: 0.437\n",
      "Iter 8/50 - Loss: 0.718   lengthscale: 0.403   noise: 0.402\n",
      "Iter 9/50 - Loss: 0.666   lengthscale: 0.371   noise: 0.370\n",
      "Iter 10/50 - Loss: 0.618   lengthscale: 0.341   noise: 0.339\n",
      "Iter 11/50 - Loss: 0.574   lengthscale: 0.313   noise: 0.311\n",
      "Iter 12/50 - Loss: 0.536   lengthscale: 0.288   noise: 0.284\n",
      "Iter 13/50 - Loss: 0.501   lengthscale: 0.267   noise: 0.259\n",
      "Iter 14/50 - Loss: 0.469   lengthscale: 0.250   noise: 0.236\n",
      "Iter 15/50 - Loss: 0.438   lengthscale: 0.236   noise: 0.215\n",
      "Iter 16/50 - Loss: 0.406   lengthscale: 0.227   noise: 0.196\n",
      "Iter 17/50 - Loss: 0.374   lengthscale: 0.220   noise: 0.178\n",
      "Iter 18/50 - Loss: 0.342   lengthscale: 0.216   noise: 0.162\n",
      "Iter 19/50 - Loss: 0.309   lengthscale: 0.214   noise: 0.147\n",
      "Iter 20/50 - Loss: 0.276   lengthscale: 0.215   noise: 0.133\n",
      "Iter 21/50 - Loss: 0.242   lengthscale: 0.217   noise: 0.121\n",
      "Iter 22/50 - Loss: 0.209   lengthscale: 0.221   noise: 0.110\n",
      "Iter 23/50 - Loss: 0.177   lengthscale: 0.227   noise: 0.100\n",
      "Iter 24/50 - Loss: 0.147   lengthscale: 0.234   noise: 0.091\n",
      "Iter 25/50 - Loss: 0.118   lengthscale: 0.242   noise: 0.082\n",
      "Iter 26/50 - Loss: 0.092   lengthscale: 0.251   noise: 0.075\n",
      "Iter 27/50 - Loss: 0.068   lengthscale: 0.260   noise: 0.068\n",
      "Iter 28/50 - Loss: 0.049   lengthscale: 0.270   noise: 0.062\n",
      "Iter 29/50 - Loss: 0.033   lengthscale: 0.279   noise: 0.057\n",
      "Iter 30/50 - Loss: 0.021   lengthscale: 0.287   noise: 0.052\n",
      "Iter 31/50 - Loss: 0.012   lengthscale: 0.293   noise: 0.048\n",
      "Iter 32/50 - Loss: 0.005   lengthscale: 0.296   noise: 0.044\n",
      "Iter 33/50 - Loss: -0.001   lengthscale: 0.296   noise: 0.041\n",
      "Iter 34/50 - Loss: -0.005   lengthscale: 0.293   noise: 0.038\n",
      "Iter 35/50 - Loss: -0.008   lengthscale: 0.288   noise: 0.036\n",
      "Iter 36/50 - Loss: -0.009   lengthscale: 0.282   noise: 0.033\n",
      "Iter 37/50 - Loss: -0.008   lengthscale: 0.275   noise: 0.032\n",
      "Iter 38/50 - Loss: -0.005   lengthscale: 0.269   noise: 0.030\n",
      "Iter 39/50 - Loss: -0.001   lengthscale: 0.263   noise: 0.029\n",
      "Iter 40/50 - Loss: 0.003   lengthscale: 0.259   noise: 0.028\n",
      "Iter 41/50 - Loss: 0.006   lengthscale: 0.256   noise: 0.027\n",
      "Iter 42/50 - Loss: 0.009   lengthscale: 0.254   noise: 0.027\n",
      "Iter 43/50 - Loss: 0.011   lengthscale: 0.253   noise: 0.026\n",
      "Iter 44/50 - Loss: 0.011   lengthscale: 0.254   noise: 0.026\n",
      "Iter 45/50 - Loss: 0.011   lengthscale: 0.255   noise: 0.026\n",
      "Iter 46/50 - Loss: 0.009   lengthscale: 0.258   noise: 0.027\n",
      "Iter 47/50 - Loss: 0.007   lengthscale: 0.261   noise: 0.027\n",
      "Iter 48/50 - Loss: 0.004   lengthscale: 0.265   noise: 0.027\n",
      "Iter 49/50 - Loss: 0.001   lengthscale: 0.269   noise: 0.028\n",
      "Iter 50/50 - Loss: -0.002   lengthscale: 0.273   noise: 0.029\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "if HAVE_KEOPS:\n",
    "    test_n = 50000\n",
    "else:\n",
    "    test_n = 10000\n",
    "\n",
    "test_x = torch.linspace(0, 1, test_n)\n",
    "if torch.cuda.is_available():\n",
    "    test_x = test_x.cuda()\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with CIQ\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:227\u001b[0m, in \u001b[0;36mMultivariateNormal.rsample\u001b[1;34m(self, sample_shape, base_samples)\u001b[0m\n\u001b[0;32m    224\u001b[0m     num_samples \u001b[38;5;241m=\u001b[39m sample_shape\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# Get samples\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mcovar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_mean_mvn_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    228\u001b[0m     res \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mview(sample_shape \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2720\u001b[0m, in \u001b[0;36mLinearOperator.zero_mean_mvn_samples\u001b[1;34m(self, num_samples)\u001b[0m\n\u001b[0;32m   2718\u001b[0m     base_samples \u001b[38;5;241m=\u001b[39m base_samples\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m   2719\u001b[0m     base_samples \u001b[38;5;241m=\u001b[39m base_samples\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 2720\u001b[0m     solves, weights, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcontour_integral_quad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2721\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_contour_quadrature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_contour_quadrature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2725\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (solves \u001b[38;5;241m*\u001b[39m weights)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\linear_operator\\utils\\contour_integral_quad.py:52\u001b[0m, in \u001b[0;36mcontour_integral_quad\u001b[1;34m(linear_op, rhs, inverse, weights, shifts, max_lanczos_iter, num_contour_quadrature, shift_offset)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rhs\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# if not inverse:\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m rhs \u001b[38;5;241m=\u001b[39m \u001b[43msqrt_precond_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shifts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;66;03m# Determine if init_vecs has extra_dimensions\u001b[39;00m\n\u001b[0;32m     56\u001b[0m     num_extra_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, rhs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m-\u001b[39m linear_op\u001b[38;5;241m.\u001b[39mdim())\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\linear_operator\\utils\\contour_integral_quad.py:46\u001b[0m, in \u001b[0;36mcontour_integral_quad.<locals>.sqrt_precond_matmul\u001b[1;34m(rhs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msqrt_precond_matmul\u001b[39m(rhs):\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preconditioner_lt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m         solves, weights, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcontour_integral_quad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreconditioner_lt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (solves \u001b[38;5;241m*\u001b[39m weights)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\linear_operator\\utils\\contour_integral_quad.py:67\u001b[0m, in \u001b[0;36mcontour_integral_quad\u001b[1;34m(linear_op, rhs, inverse, weights, shifts, max_lanczos_iter, num_contour_quadrature, shift_offset)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(), torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     66\u001b[0m     warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, NumericalWarning)  \u001b[38;5;66;03m# Supress CG stopping warning\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     _, lanczos_mat \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_cg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrhs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanczos_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tridiag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_lanczos_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tridiag_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_lanczos_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreconditioner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreconditioner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     lanczos_mat \u001b[38;5;241m=\u001b[39m lanczos_mat\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# We have an extra singleton batch dimension from the Lanczos init\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03mK^{-1/2} b = 2/pi \\int_0^\\infty (K - t^2 I)^{-1} dt\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03mWe'll approximate this integral as a sum using quadrature\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mWe'll determine the appropriate values of t, as well as their weights using elliptical integrals\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\felip\\anaconda3\\envs\\bo-cuda\\Lib\\site-packages\\linear_operator\\utils\\linear_cg.py:304\u001b[0m, in \u001b[0;36mlinear_cg\u001b[1;34m(matmul_closure, rhs, n_tridiag, tolerance, eps, stop_updating_after, max_iter, max_tridiag_iter, initial_guess, preconditioner)\u001b[0m\n\u001b[0;32m    299\u001b[0m residual_norm\u001b[38;5;241m.\u001b[39mmasked_fill_(rhs_is_zero, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    300\u001b[0m torch\u001b[38;5;241m.\u001b[39mlt(residual_norm, stop_updating_after, out\u001b[38;5;241m=\u001b[39mhas_converged)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    303\u001b[0m     k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10\u001b[39m, max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(residual_norm\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m<\u001b[39m tolerance)\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (n_tridiag \u001b[38;5;129;01mand\u001b[39;00m k \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_tridiag_iter, max_iter \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    306\u001b[0m ):\n\u001b[0;32m    307\u001b[0m     tolerance_reached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "\n",
    "test_x.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    # All relevant settings for using CIQ.\n",
    "    #   ciq_samples(True) - Use CIQ for sampling\n",
    "    #   num_contour_quadrature(10) -- Use 10 quadrature sites (Q in the paper)\n",
    "    #   minres_tolerance -- error tolerance from minres (here, <0.01%).\n",
    "    print(\"Running with CIQ\")\n",
    "    with gpytorch.settings.ciq_samples(True), gpytorch.settings.num_contour_quadrature(10), gpytorch.settings.minres_tolerance(1e-4):\n",
    "        %time y_samples = [observed_pred.rsample() for i in range(2000)]\n",
    "\n",
    "    # print(\"Running with Cholesky\")\n",
    "    # # Make sure we use Cholesky\n",
    "    # with gpytorch.settings.fast_computations(covar_root_decomposition=False):\n",
    "    #     %time y_samples = observed_pred.rsample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_samples.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
