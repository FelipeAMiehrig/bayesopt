{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.quasirandom import SobolEngine\n",
    "from hydra.utils import instantiate\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import hydra\n",
    "import torch\n",
    "import botorch\n",
    "import wandb\n",
    "from botorch.models.transforms import Standardize\n",
    "from botorch.models.transforms.input import Normalize\n",
    "from mgp_models.fully_bayesian import  MGPFullyBayesianSingleTaskGP\n",
    "from mgp_models.fit_fully_bayesian import fit_fully_bayesian_mgp_model_nuts, fit_partially_bayesian_mgp_model\n",
    "from mgp_models.utils import *\n",
    "from mgp_models.acquisition import *\n",
    "from mgp_models.test_functions import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "from botorch.acquisition.monte_carlo import MCAcquisitionFunction\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.base import MCSampler\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils import t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "import torch\n",
    "\n",
    "from botorch.acquisition import AnalyticAcquisitionFunction\n",
    "from mgp_models.fully_bayesian import  MGPFullyBayesianSingleTaskGP\n",
    "from botorch.posteriors.fully_bayesian import GaussianMixturePosterior, MCMC_DIM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkwargs = {\n",
    "\"device\": torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\"dtype\": torch.double,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SALHellingerMMAcquisitionFunction(AnalyticAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MGPFullyBayesianSingleTaskGP,\n",
    "        maximize: bool = True,\n",
    "        ll: Optional[Tensor] = None\n",
    "    ) -> None:\n",
    "        # we use the AcquisitionFunction constructor, since that of\n",
    "        # AnalyticAcquisitionFunction performs some validity checks that we don't want here\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.ll = ll\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "\n",
    "\n",
    "        posterior = self.model.posterior(X, ll= self.ll)\n",
    "        n_models = posterior._mean.shape[MCMC_DIM]\n",
    "        mean_minus_mgpmean = posterior._mean - posterior.mixture_mean.repeat(n_models,1,1)\n",
    "        BQBC = mean_minus_mgpmean.pow(2).sum(dim=MCMC_DIM)\n",
    "        var = posterior._variance.sum(dim=MCMC_DIM)\n",
    "        mixture_variance = BQBC + var\n",
    "        sigma_1 = mixture_variance.repeat(n_models,1,1)\n",
    "        mixture_mean = posterior._mean.sum(dim=MCMC_DIM)\n",
    "        mu_1 = posterior.mixture_mean.repeat(n_models,1,1) #mixture_mean.repeat(n_models,1,1)\n",
    "        sigma_2 = posterior.variance\n",
    "        mu_2 = posterior.mean\n",
    "        up = 2*torch.sqrt(sigma_1)*torch.sqrt(sigma_2)\n",
    "        down = sigma_1+sigma_2\n",
    "        to_sqrt = up.div(down)\n",
    "        sqrted = torch.sqrt(to_sqrt)\n",
    "        mean_up = mu_1 - mu_2\n",
    "        mean_up = mean_up.pow(2)\n",
    "        exped = torch.exp(-0.25*mean_up.div(down))\n",
    "        right = sqrted* exped\n",
    "        hellinger = 1 - right\n",
    "        return hellinger.mul(posterior.shaped_weights).sum(dim=MCMC_DIM)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SALWassersteinMMAcquisitionFunction(AnalyticAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MGPFullyBayesianSingleTaskGP,\n",
    "        maximize: bool = True,\n",
    "        ll: Optional[Tensor] = None\n",
    "    ) -> None:\n",
    "        # we use the AcquisitionFunction constructor, since that of\n",
    "        # AnalyticAcquisitionFunction performs some validity checks that we don't want here\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.ll = ll\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "\n",
    "\n",
    "        posterior = self.model.posterior(X, ll= self.ll)\n",
    "        n_models = posterior._mean.shape[MCMC_DIM]\n",
    "        mean_minus_mgpmean = posterior._mean - posterior.mixture_mean.repeat(n_models,1,1)\n",
    "        BQBC = mean_minus_mgpmean.pow(2).sum(dim=MCMC_DIM)\n",
    "        var = posterior._variance.sum(dim=MCMC_DIM)\n",
    "        mixture_variance = BQBC + var\n",
    "        sigma_1 = mixture_variance.repeat(n_models,1,1)\n",
    "        mixture_mean = posterior._mean.sum(dim=MCMC_DIM)\n",
    "        mu_1 = posterior.mixture_mean.repeat(n_models,1,1) #mixture_mean.repeat(n_models,1,1)\n",
    "        sigma_2 = posterior.variance\n",
    "        mu_2 = posterior.mean\n",
    "        diff_means = mu_1-mu_2\n",
    "        diff_stds = torch.sqrt(sigma_2) - torch.sqrt(sigma_1)\n",
    "        wasserstein = torch.sqrt(diff_means.pow(2)+diff_stds.pow(2))\n",
    "        return wasserstein.mul(posterior.shaped_weights).sum(dim=MCMC_DIM)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BALDKLMMAcquisitionFunction(AnalyticAcquisitionFunction):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: MGPFullyBayesianSingleTaskGP,\n",
    "        maximize: bool = True,\n",
    "        ll: Optional[Tensor] = None\n",
    "    ) -> None:\n",
    "        # we use the AcquisitionFunction constructor, since that of\n",
    "        # AnalyticAcquisitionFunction performs some validity checks that we don't want here\n",
    "        super(AnalyticAcquisitionFunction, self).__init__(model)\n",
    "        self.maximize = maximize\n",
    "        self.ll = ll\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "\n",
    "\n",
    "        posterior = self.model.posterior(X, ll= self.ll)\n",
    "        n_models = posterior._mean.shape[MCMC_DIM]\n",
    "        mean_minus_mgpmean = posterior._mean - posterior.mixture_mean.repeat(n_models,1,1)\n",
    "        BQBC = mean_minus_mgpmean.pow(2).sum(dim=MCMC_DIM)\n",
    "        var = posterior._variance.sum(dim=MCMC_DIM)\n",
    "        mixture_variance = BQBC + var\n",
    "        sigma_1 = mixture_variance.repeat(n_models,1,1)\n",
    "        mixture_mean = posterior._mean.sum(dim=MCMC_DIM)\n",
    "        mu_1 = posterior.mixture_mean.repeat(n_models,1,1) #mixture_mean.repeat(n_models,1,1)\n",
    "        sigma_2 = posterior.variance\n",
    "        mu_2 = posterior.mean\n",
    "        left = torch.log(torch.sqrt(sigma_2).div(torch.sqrt(sigma_1)))\n",
    "        dif_means = mu_1-mu_2\n",
    "        up = sigma_1 + dif_means.pow(2)\n",
    "        KL = left + up.div(2*sigma_2) - 0.5\n",
    "        return KL.mul(posterior.shaped_weights).sum(dim=MCMC_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_yhat(gp, test_X, tkwargs, batch_size = 100):\n",
    "\n",
    "    total_batches = test_X.size(0) // batch_size\n",
    "    Y_full = torch.Tensor().to(**tkwargs)\n",
    "    for i in range(total_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_X = test_X[start_idx:end_idx]\n",
    "        posterior = gp.posterior(batch_X)\n",
    "        Y_hat = posterior.mean\n",
    "        Y_full = torch.cat((Y_full, Y_hat),1)\n",
    "    return Y_full\n",
    "import seaborn as sns  # Import seaborn for KDE plot\n",
    "\n",
    "\n",
    "def plot_gps(test_X, std, synthetic_function, Y_hat, X_train, Y_train, acq_values=None, weights=None):\n",
    "    test_X = convert_bounds(test_X, BOUNDS, DIM)\n",
    "    x = test_X.detach().squeeze().numpy()\n",
    "    y = synthetic_function.evaluate_true(test_X).numpy()\n",
    "    X_train = convert_bounds(X_train, BOUNDS, DIM)\n",
    "    x_points = X_train.detach().squeeze().numpy()\n",
    "    y_points = Y_train.detach().numpy()\n",
    "    if acq_values is not None:\n",
    "        acq_values = acq_values.detach().squeeze().numpy()\n",
    "\n",
    "    gps_y = Y_hat.detach().squeeze().numpy()\n",
    "    ci = 1.96 * std\n",
    "\n",
    "    df = pd.DataFrame({'x': x, \"y\": y, 'y_lower': y - ci, 'y_upper': y + ci})\n",
    "    if acq_values is not None:\n",
    "        df['acq_values'] = acq_values\n",
    "\n",
    "    gp_cols = ['gp_' + str(i) for i in range(len(gps_y))]\n",
    "    for i, gp_col in enumerate(gp_cols):\n",
    "        df[gp_col] = gps_y[i]\n",
    "\n",
    "    df = df.sort_values(by='x', ascending=True).reset_index(drop=True)\n",
    "\n",
    "    if weights is not None:\n",
    "        norm = plt.Normalize(min(weights), max(weights))\n",
    "        cmap = plt.cm.viridis\n",
    "\n",
    "    # Create a figure with two subplots, sharing the x-axis\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})\n",
    "\n",
    "    ax1.plot(df.x, df.y, label='True function', color='red')\n",
    "    ax1.fill_between(df.x, df.y_lower, df.y_upper, color='red', alpha=0.3, label='95% CI')\n",
    "    for i, gp_col in enumerate(gp_cols):\n",
    "    #    if weights is None or weights[i] > 0:\n",
    "        color = cmap(norm(weights[i])) if weights is not None else 'blue'\n",
    "        ax1.plot(df.x, df[gp_col], color=color)\n",
    "    ax1.scatter(x_points, y_points, color='gold', s=200, marker='*', label='Queried points', zorder=5)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Y')\n",
    "\n",
    "    # Determine the limits for the x-axis based on the upper plot\n",
    "    x_min, x_max = ax1.get_xlim()\n",
    "\n",
    "    # Plotting the KDE for acq_values on the second subplot (ax2) with x-axis limits\n",
    "    if acq_values is not None:\n",
    "        sns.kdeplot(x=df.x, weights=df.acq_values, ax=ax2, fill=True, color='green', clip=(x_min, x_max), bw_adjust=0.5)\n",
    "        ax2.set_xlim(x_min, x_max)  # Apply the same x-axis limits as the upper plot\n",
    "        ax2.set_ylabel('Acquisition Value Density')\n",
    "        ax2.set_xlabel('X')\n",
    "\n",
    "    plt.suptitle('Sine Function with Various Levels of Noise and 95% CI for the Original')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 1\n",
    "BOUNDS =  [[0, 20]]\n",
    "type = 'part_bayesian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "synthetic_function = Higdon(noise_std=0.1).to(**tkwargs)\n",
    "bounds = BOUNDS\n",
    "    #print(bounds)\n",
    "X = SobolEngine(dimension=DIM, scramble=True).draw(3).to(**tkwargs)\n",
    "    #print(X)\n",
    "X_scaled = convert_bounds(X, BOUNDS, DIM)\n",
    "Y = synthetic_function(X_scaled).unsqueeze(-1)\n",
    "poolU = get_candidate_pool(dim=DIM, bounds=BOUNDS, size=10000).to(**tkwargs)\n",
    "ewig_pool = poolU.clone()\n",
    "X_test, Y_test = get_test_set(synthetic_function=synthetic_function, \n",
    "                                  bounds=bounds, \n",
    "                                  dim=DIM, \n",
    "                                  noise_std=0.1,\n",
    "                                  size=10000)  \n",
    "     \n",
    "X_test, Y_test = X_test.to(**tkwargs), Y_test.to(**tkwargs)\n",
    "dict_best_params = None\n",
    "for i in range(60):\n",
    "    train_Y = Y  # Flip the sign since we want to minimize f(x)\n",
    "    gp = MGPFullyBayesianSingleTaskGP(\n",
    "        train_X=X, \n",
    "        train_Y=train_Y, \n",
    "        #train_Yvar=torch.full_like(train_Y, 1e-6),\n",
    "        #input_transform=Normalize(d=cfg.functions.dim, bounds=bountensor_scaledds),\n",
    "        outcome_transform=Standardize(m=1)\n",
    "    )\n",
    "    if type == 'part_bayesian':\n",
    "        ll = fit_partially_bayesian_mgp_model(model=gp,\n",
    "                                                num_samples= 32,\n",
    "                                                lr=0.1,\n",
    "                                                learning_steps=100,\n",
    "                                                print_iter=False,\n",
    "                                                plot=False, \n",
    "                                                dict_params=dict_best_params)\n",
    "        dict_best_params = get_best_model_params(gp, ll=ll)\n",
    "        print(dict_best_params)\n",
    "    else:\n",
    "        ll = fit_fully_bayesian_mgp_model_nuts(gp,\n",
    "                                            warmup_steps=256,\n",
    "                                            num_samples=128 ,\n",
    "                                            thinning=6,\n",
    "                                            disable_progbar=False)\n",
    "    acq_function = BQBCAcquisitionFunction(gp, ll=ll)\n",
    "    if ll is not None:\n",
    "        ll = ll.detach()\n",
    "        likelihoods = ll.exp()\n",
    "        #median_val = np.percentile(likelihoods, 100)\n",
    "        #print(median_val)\n",
    "\n",
    "        #likelihoods[likelihoods < median_val] = 0\n",
    "        weights = likelihoods.pow(1).squeeze().div(likelihoods.pow(1).sum())\n",
    "        npll = weights.numpy()\n",
    "        norm = plt.Normalize(npll.min(), npll.max())\n",
    "        colors = plt.cm.viridis(norm(npll))\n",
    "\n",
    "    # Generate a sequence of colors from the 'viridis' colormap\n",
    "\n",
    "    # Plotting\n",
    "        plt.bar(range(len(npll)), npll, color=colors)\n",
    "        plt.ylabel('Value')\n",
    "        plt.xlabel('Index')\n",
    "        plt.title('Bar Plot of Tensor Values with Viridis Palette')\n",
    "        plt.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='viridis'), ax=plt.gca(), label='Value')\n",
    "        plt.show()\n",
    "    else:\n",
    "        weights = ll\n",
    "\n",
    "    acq_values = acq_function(poolU)\n",
    "    acq_values_all = acq_function(ewig_pool)\n",
    "    best_index = torch.argmax(acq_values)\n",
    "    candidates = poolU[best_index].unsqueeze(-1)\n",
    "    new_pool = torch.cat((poolU[:best_index], poolU[best_index + 1:]), dim=0)\n",
    "    candidates, best_acq_value, poolU = candidates.T, acq_values[best_index].unsqueeze(-1), new_pool\n",
    "\n",
    "    candidates_scaled = convert_bounds(candidates, BOUNDS, DIM)\n",
    "    y_hat = get_yhat(gp, X_test, tkwargs)\n",
    "    plot_gps(X_test, 0.1, synthetic_function, y_hat, X, Y,acq_values_all, weights)\n",
    "    Y_next = synthetic_function(candidates_scaled).unsqueeze(-1)\n",
    "    if DIM ==1:\n",
    "        Y_next=Y_next.unsqueeze(-1)\n",
    "    Y = torch.cat((Y, Y_next)).to(**tkwargs)\n",
    "    X = torch.cat((X, candidates)).to(**tkwargs)\n",
    "    print(acq_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=2\n",
    "gp.covar_module.base_kernel.lengthscale[0].fill_(2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import stats\n",
    "def mode_params(gp):\n",
    "    model_dict = gp.get_param_dict()\n",
    "    decomposed_param_dict = {}\n",
    "    decomposed_param_dict['noise'] = model_dict['noise'].detach().squeeze().numpy()\n",
    "    decomposed_param_dict['outputscale']  = model_dict['outputscale'].detach().squeeze().numpy()\n",
    "    for i in range(model_dict['lengthscale'].size()[1]):\n",
    "        decomposed_param_dict['legthscale_'+str(i)] = model_dict['lengthscale'][:,i].squeeze().numpy()\n",
    "    decomposed_param_dict['mean']  = model_dict['mean'].numpy()\n",
    "    df_params = pd.DataFrame(decomposed_param_dict)\n",
    "    print(df_params.head())\n",
    "    array_params = df_params.values.T\n",
    "    kernel = stats.gaussian_kde(array_params)\n",
    "    print(kernel([0.3,0.1,2,0.3]))\n",
    "    x0 = np.mean(array_params, axis=1)\n",
    "    print(x0)\n",
    "    bounds = [(1e-8, None) for i in range(array_params.shape[0]-1)]\n",
    "    bounds.append((None,None))\n",
    "    opt = optimize.minimize(lambda *args: -kernel(args), x0 =x0, method='L-BFGS-B', tol=1e-6, bounds= bounds)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_params(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = gp.get_param_dict()\n",
    "decomposed_param_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_param_dict['noise'] = model_dict['noise'].detach().squeeze().numpy()\n",
    "decomposed_param_dict['outputscale']  = model_dict['outputscale'].detach().squeeze().numpy()\n",
    "decomposed_param_dict['mean']  = model_dict['mean'].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['lengthscale'][:,0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(model_dict['lengthscale'].size()[1]):\n",
    "    decomposed_param_dict['legthscale_'+str(i)] = model_dict['lengthscale'][:,i].squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params = pd.DataFrame(decomposed_param_dict)\n",
    "array_params = df_params.values.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = stats.gaussian_kde(array_params)\n",
    "# Minimize the negative instead of maximizing\n",
    "# Depending on the shape of your data, you might want to set some bounds\n",
    "#opt = optimize.minimize_scalar(lambda x: -kernel(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel([0,0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(array_params.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = [(1e-8, None) for i in range(array_params.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_kde_value(x):\n",
    "    return -kernel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimize.minimize(lambda *args: -kernel(args), x0 =x0, method='L-BFGS-B', tol=1e-6, bounds= bounds)\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-kernel(array_params[:,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_params[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_yhat(gp, test_X, tkwargs, batch_size = 100):\n",
    "\n",
    "    total_batches = test_X.size(0) // batch_size\n",
    "    Y_full = torch.Tensor().to(**tkwargs)\n",
    "    for i in range(total_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_X = test_X[start_idx:end_idx]\n",
    "        posterior = gp.posterior(batch_X)\n",
    "        Y_hat = posterior.mean\n",
    "        Y_full = torch.cat((Y_full, Y_hat),1)\n",
    "    return Y_full\n",
    "\n",
    "def plot_gps(test_X, std, synthetic_function, Y_hat, X_train, Y_train, ll = None):\n",
    "\n",
    "        # Define the sine function\n",
    "    test_X = convert_bounds(test_X,BOUNDS, DIM)\n",
    "    x = test_X.detach().squeeze().numpy()\n",
    "    #y = test_Y.detach().numpy()\n",
    "    y = synthetic_function.evaluate_true(test_X).numpy()\n",
    "    X_train = convert_bounds(X_train,BOUNDS, DIM)\n",
    "    x_points = X_train.detach().squeeze().numpy()\n",
    "    y_points = Y_train.detach().numpy()\n",
    "\n",
    "    # Generate noisy versions of y\n",
    "    gps_y = Y_hat.detach().squeeze().numpy()\n",
    "    \n",
    "    ci = 1.96 * std\n",
    "\n",
    "    df = pd.DataFrame({'x': x, \"y\": y})\n",
    "    df['y_lower'] = df.y - ci\n",
    "    df['y_upper'] = df.y + ci\n",
    "\n",
    "    gp_cols = []\n",
    "    for i in range(len(gps_y)):\n",
    "        gp_col = 'gp_' + str(i)\n",
    "        df[gp_col] = gps_y[i]\n",
    "        gp_cols.append(gp_col)\n",
    "    df = df.sort_values(by='x', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the ll values to 0-1 for color mapping\n",
    "    if ll is not None:\n",
    "        ll = ll.exp().detach().squeeze().div(ll.exp().detach().sum())\n",
    "        #ll = ll.detach().numpy()\n",
    "        norm = plt.Normalize(min(ll), max(ll))\n",
    "        cmap = plt.cm.viridis  # Choose a colormap\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.x, df.y, label='true function', color='red')\n",
    "    plt.fill_between(df.x, df.y_lower, df.y_upper, color='red', alpha=0.3, label='95% CI')\n",
    "\n",
    "    for i, gp_col in enumerate(gp_cols):\n",
    "        color = cmap(norm(ll[i])) if ll is not None else 'blue'  # Use ll value for color if ll is provided\n",
    "        plt.plot(df.x, df[gp_col], color=color)\n",
    "    \n",
    "    plt.scatter(x_points, y_points, color='gold', s=200, marker='*', label='queried points', zorder=30)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Sine Function with Various Levels of Noise and 96% CI for the Original')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
